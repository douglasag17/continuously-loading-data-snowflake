# [Demo] Streaming data from a transactional database to a data warehouse using Kafka (Confluent Cloud) and Snowflake.
Continuously loading data to your data warehouse with Snowflake + Declarative data pipelines with Dynamic tables.

## Architecture Overview
![architecture](/images/architecture.png)

TODO: link to demo recording

TODO: publish this as a blog post in Github pages (https://pages.github.com/) / Medium / Factored Post ?

TODO: Insert Images for everything

## Why Streaming data pipelines?
[Get your data to the right place, in the right format, at the right time to build data products faster and unlock endless use cases.](https://www.confluent.io/streaming-data-pipelines/) 
https://assets.confluent.io/m/1980f4092c35e334/original/20230125-EB-Transform_Your_Data_Pipelines.pdf 

Analyzing the events in real time—​as opposed to batch—​gives the flexibility to see outcomes as they occur or in a windowed fashion depending on the consuming application.

[stream processing vs batch processing](https://www.confluent.io/learn/batch-vs-real-time-data-processing/#:~:text=weekly%20or%20monthly.-,Streaming%20data%20processing,-happens%20as%20the)

All industries that are generating data continuously will benefit from processing streaming data. The use cases typically start from internal IT systems monitoring and reporting like collecting the data streams generated by employees interacting with their web browser and devices and the data generated by its applications and servers. The operations of the company and its products benefit from data stream processing of sensors, equipment, data centers and many more sources.

Since its customers and partners also consume and process streaming data, the ability to send, receive, process streaming data becomes increasingly important. As more companies rely on its data, its ability to process, analyze, apply machine learning and artificial intelligence to streaming data is crucial.


## Setting up AWS RDS (PostgreSQL)
TODO: Pensar en como hacer que la audiencia interactue generando eventos. Por ejemplo hacer una [encuesta](https://developer.confluent.io/tutorials/survey-responses/ksql.html) y que las respuestas se almacenen en la db (API???)

Data model of the transactional database

![datamodel](/images/transactional-data-model.png)

[Please follow](aws/README.md)




### Deploying API on AWS (Lambda, API Gateway)
TODO: [API Example for surveys](https://github.com/eneacosta/survey-fastapi-mongo/blob/master/README.md)

### Test with mock data
Python script to write to an API that writes to a DB.

## Setting up Snowflake
- Sign up, etc
- Create database
    ```
    USE ROLE ACCOUNTADMIN;

    CREATE DATABASE KAFKA_DB;

    USE DATABASE KAFKA_DB;
    ```

## Setting up Confluent Cloud
Confluent Cloud is a fully managed service for Apache Kafka, a distributed streaming platform technology. It provides a single source of truth across event streams that mission-critical applications can rely on.

With Confluent Cloud, developers can easily get started with serverless Kafka and the related services required to build event streaming applications, including fully managed connectors, Schema Registry, and ksqlDB for stream processing

- Sign up, etc

- Create a new cluster

- Generate a new API Key for your Kafka Cluster
- Download it and keep it

## Setting up Postgres CDC Source connector

Add Postgres CDC Source connector
- Kafka API Key
  
- Database name: postgres
- Database server name: postgres_cdc
- SSl mode: disable
- host
- port
- username
- password
  
- Output Kafka record value format -> JSON
- Output Kafka record key format -> JSON
  
- Include all the tables
- A Topic is created per table by the Connector


## Setting up ksqlDB (Stream processing)

For a topic producing data from an OLTP -> https://developer.confluent.io/learn-kafka/data-pipelines/hands-on-joining-data-streams/ 
- Create a Stream per data producer (topic). Notice that sensor is marked with the KEY keyword.
  ```SQL
  CREATE STREAM CUSTOMERS_S
  WITH (KAFKA_TOPIC ='mysql01.demo.CUSTOMERS',
        KEY_FORMAT  ='JSON',
        VALUE_FORMAT='AVRO'
  );
  ```
- Now create a ksqlDB table on the customer data. A ksqlDB table is built on a stream and returns the value for a given key. If there are two messages with the same key, the table will have one entry (rather than two, as in a stream).
  ```SQL
  CREATE TABLE CUSTOMERS WITH (FORMAT='AVRO') AS
    SELECT id                            AS customer_id,
           LATEST_BY_OFFSET(first_name)  AS first_name,
           LATEST_BY_OFFSET(last_name)   AS last_name,
           LATEST_BY_OFFSET(email)       AS email,
           LATEST_BY_OFFSET(club_status) AS club_status
    FROM   CUSTOMERS_S
    GROUP BY id;
  ```
- Join both, the stream and the table by creating a new stream that writes to a new topic. An [inner table-table join produces a table](https://developer.confluent.io/learn-kafka/inside-ksqldb/streaming-joins/), unlike the stream-table joins you have seen so far. It has two buffers—one for each table—and it fires when there is an update on either side of the expression.
  ```SQL
  CREATE STREAM RATINGS_WITH_CUSTOMER_DATA
          WITH (KAFKA_TOPIC='ratings-enriched') AS
    SELECT C.CUSTOMER_ID,
          C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME,
          C.CLUB_STATUS,
          C.EMAIL,
          R.RATING_ID,
          R.MESSAGE,
          R.STARS,
          R.CHANNEL,
          TIMESTAMPTOSTRING(R.ROWTIME,'yyyy-MM-dd''T''HH:mm:ss.SSSZ') AS RATING_TS
  FROM   RATINGS_LIVE R
          INNER JOIN CUSTOMERS C
            ON R.USER_ID = C.CUSTOMER_ID
  EMIT CHANGES;
  ```
  [Streams are unbounded series of events, while tables are the current state of a given key.](https://developer.confluent.io/learn-kafka/ksqldb/streams-and-tables/#:~:text=Streams%20are%20unbounded%20series%20of,want%20to%20use%20the%20data.)

  ```SQL
  topic_payroll
  {
    "employee_id": 1081,
    "first_name": "Sheridan",
    "last_name": "Lezemere",
    "age": 48,
    "ssn": "674-68-7406",
    "hourly_rate": 23,
    "gender": "male",
    "email": "jpbi@mycompany.com"
  }

  topic_ksql_1
  {
    "employee_id": 1055,
    "bonus": 25,
    "ts": 1640317836271
  }

  -- Create Streams
  CREATE STREAM PAYROLL_STREAM (
    EMPLOYEE_ID INTEGER KEY,
    FIRST_NAME VARCHAR,
    LAST_NAME VARCHAR,
    AGE INTEGER,
    SSN VARCHAR,
    HOURLY_RATE INTEGER,
    GENDER VARCHAR,
    EMAIL VARCHAR
  )
  WITH (
    KAFKA_TOPIC = 'topic_payroll',
    VALUE_FORMAT='JSON'
  )
  ;

  CREATE STREAM PAYROLL_BONUS_STREAM (
    EMPLOYEE_ID INTEGER KEY,
    BONUS INTEGER,
    TS INTEGER
  )
  WITH (
    KAFKA_TOPIC = 'topic_ksql_1',
    VALUE_FORMAT='JSON'
  )
  ;

  -- Verify Streams: Let’s now query this data, along with the data that we inserted earlier. Because we want to see all of the data that’s already in the stream, and not just new messages as they arrive, we need to run:
  -- ensure topics are read from beginning
  SET 'auto.offset.reset' = 'earliest';

  SELECT * 
  FROM PAYROLL_STREAM 
  EMIT CHANGES;

  SELECT * 
  FROM PAYROLL_BONUS_STREAM 
  EMIT CHANGES;

  -- Create Tables
  CREATE TABLE PAYROLL_TABLE 
  WITH (VALUE_FORMAT='JSON') AS
  SELECT
    EMPLOYEE_ID,
    LATEST_BY_OFFSET(FIRST_NAME) AS FIRST_NAME,
    LATEST_BY_OFFSET(LAST_NAME) AS LAST_NAME,
    LATEST_BY_OFFSET(AGE) AS AGE,
    LATEST_BY_OFFSET(SSN) AS SSN,
    LATEST_BY_OFFSET(HOURLY_RATE) AS HOURLY_RATE,
    LATEST_BY_OFFSET(GENDER) AS GENDER,
    LATEST_BY_OFFSET(EMAIL) AS EMAIL
  FROM PAYROLL_STREAM
  GROUP BY EMPLOYEE_ID
  EMIT CHANGES;

  -- Verify table
  SHOW TABLES;

  -- Joining the table and stream
  CREATE STREAM PAYROLL_COMPLETE_STREAM 
  WITH (
    KAFKA_TOPIC='topic_payroll_complete', 
    VALUE_FORMAT='JSON',
    PARTITIONS=1
  ) AS
  SELECT 
    P.EMPLOYEE_ID,
    P.FIRST_NAME + ' ' + P.LAST_NAME AS FULL_NAME,
    P.AGE,
    P.SSN,
    P.HOURLY_RATE,
    PB.BONUS,
    P.GENDER,
    P.EMAIL
  FROM PAYROLL_BONUS_STREAM PB
  INNER JOIN PAYROLL_TABLE P
  ON PB.EMPLOYEE_ID = P.EMPLOYEE_ID
  EMIT CHANGES;

  SELECT * 
  FROM PAYROLL_COMPLETE_STREAM 
  EMIT CHANGES;
  ```

## Setting up Snowflake Sink Kafka Connector

Steps:
1. [Overview of the Kafka Connector](https://docs.snowflake.com/en/user-guide/kafka-connector-overview) 
2. Go to Confluent Cloud -> Connectors -> Snowflake Sink
3. Select the Topic from where you want to consume messages
4. Use your existing API key of your Kafka Cluster
5. [Generate a Snowflake key pair](https://docs.confluent.io/cloud/current/connectors/cc-snowflake-sink.html#generate-a-snowflake-key-pair)
   - Open up a Terminal and generate a private and public key using OpenSSL.
      ```bash
      openssl genrsa -out snowflake_key.pem 2048
      ```
      ```bash
      openssl rsa -in snowflake_key.pem  -pubout -out snowflake_key.pub
      ```
      ```bash
      ls -l snowflake_key*
      ```
1. Fill out required parameters
   - Connection URL: Go to Snowflake -> Admin -> Accounts -> Locator
   - Connection User Name: Snowflake -> Admin -> Accounts -> Users & Roles
    - Add public key to the Snowflake user by running:
      - `grep -v "BEGIN PUBLIC" snowflake_key.pub | grep -v "END PUBLIC"|tr -d '\r\n'`
      - `ALTER USER DOUGLASCONFLUENT SET RSA_PUBLIC_KEY = '<public-key>'`
   - Private Key
    - `grep -v "BEGIN RSA PRIVATE KEY" snowflake_key.pem | grep -v "END RSA PRIVATE KEY"|tr -d '\r\n'`
   - Database Name: KAFKA_DB
   - Schema Name: PUBLIC
2. A Stage, Pipe, and a Table should be created by the Kafka connector within seconds.
    ```SQL
    USE WAREHOUSE COMPUTE_WH;

    DESC STAGE SNOWFLAKE_KAFKA_CONNECTOR_LCC_PRZ15M_1550001102_STAGE_TOPIC_PAYROLL;
    LIST @SNOWFLAKE_KAFKA_CONNECTOR_LCC_PRZ15M_1550001102_STAGE_TOPIC_PAYROLL;

    DESC PIPE SNOWFLAKE_KAFKA_CONNECTOR_LCC_PRZ15M_1550001102_PIPE_TOPIC_PAYROLL_0;

    SELECT * FROM TOPIC_PAYROLL;

    CREATE OR REPLACE VIEW TOPIC_PAYROLL_VIEW AS
    SELECT 
        RECORD_CONTENT,
        RECORD_CONTENT:employee_id::NUMBER AS EMPLOYEE_ID,

        TO_TIMESTAMP_NTZ(RECORD_METADATA:CreateTime::VARCHAR) as CREATED_AT
    FROM TOPIC_PAYROLL
    ;

    SELECT * FROM TOPIC_PAYROLL_VIEW ORDER BY CREATED_AT DESC;
    ```

    TODO: Terraform to spin up cloud resources in the cloud


Stream Lineage (Confluent)
![lineage](/images/stream_lineage.png)


Data model of the analytical database

![datamodel_analytical](/images/analytical-data-model.png)


TODO: [github markdown code snippet from file](https://www.stevemar.net/github-code-in-readme/) or see [this](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-a-permanent-link-to-a-code-snippet)

Example:
https://github.com/douglasag17/data_engineering_challenge/blob/7ecbe029a7467f52adf63f1f618e50ca6088756f/code/etl/etl.py#L32C4-L36
